import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
import cv2  # ç”¨äºå¤„ç†è‡ªå®šä¹‰å›¾åƒ

# å¯ç”¨ Keras åç«¯
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

# --- 1. æ•°æ®é›†åŠ è½½ä¸é¢„å¤„ç† ---


def load_and_preprocess_data():
    """åŠ è½½ MNIST æ•°æ®é›†å¹¶è¿›è¡Œé¢„å¤„ç†"""
    print("--- 1. æ•°æ®é›†åŠ è½½ä¸é¢„å¤„ç† ---")
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    # å½’ä¸€åŒ– (å°†åƒç´ å€¼ä» 0-255 ç¼©æ”¾åˆ° 0-1)
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0

    # æ ‡ç­¾ç‹¬çƒ­ç¼–ç  (One-Hot Encoding)
    # 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
    y_train = to_categorical(y_train, num_classes=10)
    y_test = to_categorical(y_test, num_classes=10)

    # æ³¨æ„ï¼šBP/MLP éœ€è¦å±•å¹³(Flatten)è¾“å…¥ï¼ŒKeras çš„ Flatten å±‚å¯ä»¥è‡ªåŠ¨å®Œæˆ
    
    print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶ (X): {x_train.shape}")
    print(f"è®­ç»ƒæ ‡ç­¾å½¢çŠ¶ (Y): {y_train.shape}")
    
    return x_train, y_train, x_test, y_test

# --- 2. BP ç¥ç»ç½‘ç»œæ¨¡å‹æ„å»º ---


def build_bp_model(input_shape):
    """æ„å»º BP ç¥ç»ç½‘ç»œ (MLP) æ¨¡å‹"""
    print("--- 2. BP ç¥ç»ç½‘ç»œæ¨¡å‹æ„å»º ---")
    model = Sequential([
        # å±•å¹³å±‚ï¼šå°† 28x28 å›¾åƒè¾“å…¥è½¬æ¢ä¸º 784 ç»´å‘é‡
        Flatten(input_shape=input_shape), 
        
        # éšè—å±‚ 1: 512 ä¸ªç¥ç»å…ƒï¼Œä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
        Dense(512, activation='relu'), 
        
        # éšè—å±‚ 2: 256 ä¸ªç¥ç»å…ƒï¼Œä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
        Dense(256, activation='relu'), 
        
        # è¾“å‡ºå±‚: 10 ä¸ªç¥ç»å…ƒ (å¯¹åº” 0-9 åä¸ªç±»åˆ«), ä½¿ç”¨ Softmax æ¿€æ´»å‡½æ•°è¿›è¡Œæ¦‚ç‡è¾“å‡º
        Dense(10, activation='softmax') 
    ])

    # ç¼–è¯‘æ¨¡å‹ï¼š
    # ä¼˜åŒ–å™¨: Adam (é«˜æ•ˆçš„æ¢¯åº¦ä¸‹é™å˜ä½“)
    # æŸå¤±å‡½æ•°: Categorical Crossentropy (é€‚ç”¨äº One-Hot ç¼–ç çš„åˆ†ç±»é—®é¢˜)
    # è¯„ä¼°æŒ‡æ ‡: å‡†ç¡®ç‡ (Accuracy)
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    model.summary()
    return model

# --- 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ---


def train_and_evaluate_model(model, x_train, y_train, x_test, y_test, 
                             epochs=10, batch_size=128):
    """è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½"""
    print("--- 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ---")
    
    # è®­ç»ƒæ¨¡å‹
    history = model.fit(x_train, y_train,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(x_test, y_test),
                        verbose=1)

    # è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
    loss, acc = model.evaluate(x_test, y_test, verbose=0)
    print("\n--- è¯„ä¼°ç»“æœ ---")
    print(f"æµ‹è¯•é›†æŸå¤± (Loss): {loss:.4f}")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡ (Accuracy): {acc*100:.2f}%")
    
    return history

# --- 4. è¯†åˆ«ä½ æ‰‹å†™çš„æ•°å­— (æ ¸å¿ƒè¦æ±‚) ---


def preprocess_custom_image(image_path):
    """
    æ”¹è¿›ç‰ˆé¢„å¤„ç†ï¼š
    1. è¯»å–ç°åº¦å›¾
    2. äºŒå€¼åŒ–å¤„ç†ï¼ˆå»é™¤é˜´å½±å’ŒèƒŒæ™¯å™ªå£°ï¼‰
    3. å¯»æ‰¾æ•°å­—çš„æœ€å°å¤–æ¥çŸ©å½¢ï¼ˆè‡ªåŠ¨è£å‰ªï¼‰
    4. ä¿æŒæ¯”ä¾‹ç¼©æ”¾åˆ° 20x20ï¼Œç„¶åè´´åˆ° 28x28 çš„ä¸­å¿ƒ
    """
    # 1. è¯»å–å›¾åƒ
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        raise FileNotFoundError(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")

    # 2. äºŒå€¼åŒ–å¤„ç† (é»‘åº•ç™½å­—)
    # ä½¿ç”¨ OTSU ç®—æ³•è‡ªåŠ¨å¯»æ‰¾æœ€ä½³é˜ˆå€¼ï¼Œæˆ–è€…æ‰‹åŠ¨è°ƒæ•´é˜ˆå€¼
    # å¦‚æœä½ çš„å›¾æ˜¯ç™½åº•é»‘å­—ï¼Œå…ˆåè½¬
    # cv2.THRESH_BINARY_INV ä¼šè‡ªåŠ¨åè½¬é¢œè‰²ï¼ˆå˜æˆé»‘åº•ç™½å­—ï¼‰å¹¶äºŒå€¼åŒ–
    _, img_bin = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV 
                               + cv2.THRESH_OTSU)
    
    # 3. æ‰¾åˆ°æ•°å­—çš„è¾¹ç•Œ (Bounding Box)
    contours, _ = cv2.findContours(img_bin, cv2.RETR_EXTERNAL, 
                                   cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        # å¦‚æœæ²¡æ‰¾åˆ°è½®å»“ï¼Œè¿”å›å…¨é»‘å›¾åƒ
        return np.zeros((1, 28, 28), dtype='float32')
        
    # æ‰¾åˆ°æœ€å¤§çš„è½®å»“ï¼ˆå‡è®¾æœ€å¤§çš„è½®å»“æ˜¯æ•°å­—ï¼‰
    c = max(contours, key=cv2.contourArea)
    x, y, w, h = cv2.boundingRect(c)
    
    # è£å‰ªå‡ºæ•°å­—éƒ¨åˆ†
    digit = img_bin[y:y+h, x:x+w]
    
    # 4. ä¿æŒæ¯”ä¾‹ç¼©æ”¾
    # MNIST æ•°å­—é€šå¸¸åœ¨ 20x20 åƒç´ èŒƒå›´å†…ï¼Œæ”¾åœ¨ 28x28 çš„ä¸­å¿ƒ
    target_size = 20
    
    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    scale = target_size / max(h, w)
    new_w, new_h = int(w * scale), int(h * scale)
    
    resized_digit = cv2.resize(digit, (new_w, new_h), 
                               interpolation=cv2.INTER_AREA)
    
    # 5. åˆ›å»º 28x28 çš„é»‘è‰²èƒŒæ™¯å¹¶å±…ä¸­ç²˜è´´
    final_img = np.zeros((28, 28), dtype=np.uint8)
    
    # è®¡ç®—ç²˜è´´ä½ç½®ï¼ˆå±…ä¸­ï¼‰
    pad_x = (28 - new_w) // 2
    pad_y = (28 - new_h) // 2
    
    final_img[pad_y:pad_y+new_h, pad_x:pad_x+new_w] = resized_digit
    
    # 6. å½’ä¸€åŒ–
    final_img = final_img.astype('float32') / 255.0
    
    # å¢åŠ ç»´åº¦ä»¥åŒ¹é… Keras è¾“å…¥ (1, 28, 28)
    return np.expand_dims(final_img, axis=0)


def test_custom_numbers(model):
    """
    æµ‹è¯•ä½ æ‰‹å†™çš„ 10 ä¸ªæ•°å­— (0-9)ã€‚
    ä½ éœ€è¦åˆ›å»º 10 ä¸ªå›¾åƒæ–‡ä»¶ï¼Œä¾‹å¦‚ '0.png', '1.png', ..., '9.png'ï¼Œ
    å¹¶ç¡®ä¿å®ƒä»¬ä½äºå½“å‰ç›®å½•ä¸‹ï¼Œä¸”æ˜¯é»‘ç™½æ‰‹å†™æ•°å­—ã€‚
    """
    print("\n--- 4. è¯†åˆ«ä½ æ‰‹å†™çš„æ•°å­— ---")
    custom_results = {}
    
    # å®šä¹‰ 10 ä¸ªè‡ªå®šä¹‰å›¾åƒçš„æ–‡ä»¶å
    image_names = [f"{i}.png" for i in range(10)] 

    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    axes = axes.flatten()

    for i, name in enumerate(image_names):
        try:
            # é¢„å¤„ç†å›¾åƒ
            custom_img_processed = preprocess_custom_image(name)
            
            # æ¨¡å‹é¢„æµ‹
            prediction = model.predict(custom_img_processed, verbose=0)
            predicted_class = np.argmax(prediction)
            confidence = np.max(prediction) * 100
            
            # è®°å½•ç»“æœ
            result_str = f"prediction: {predicted_class} ({confidence:.2f}%)"
            is_correct = predicted_class == i
            custom_results[i] = (result_str, is_correct)
            
            # ç»˜å›¾å±•ç¤º
            ax = axes[i]
            # ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼Œæ˜¾ç¤º 28x28 å›¾åƒ
            ax.imshow(custom_img_processed[0], cmap='gray') 
            ax.set_title(f"Target: {i}\n{result_str}", 
                         color='green' if is_correct else 'red')
            ax.axis('off')

        except FileNotFoundError:
            print(f"â— è­¦å‘Š: ç¼ºå°‘æ–‡ä»¶ {name}ã€‚è¯·åˆ›å»º 10 ä¸ªæ‰‹å†™æ•°å­—å›¾åƒæ–‡ä»¶ (0.png - 9.png)ã€‚")
            custom_results[i] = ("æ–‡ä»¶ç¼ºå¤±", False)
            axes[i].set_title(f"Target: {i}\næ–‡ä»¶ç¼ºå¤±", color='blue')
            axes[i].axis('off')
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {name} æ—¶å‡ºé”™: {e}")
            custom_results[i] = (f"å¤„ç†é”™è¯¯: {e}", False)
            axes[i].set_title(f"Target: {i}\nå¤„ç†é”™è¯¯", color='red')
            axes[i].axis('off')
    
    plt.tight_layout()
    plt.show()

    # æ€»ç»“è¯†åˆ«ç»“æœ
    print("\n--- è¯†åˆ«ç»“æœæ€»ç»“ ---")
    all_correct = True
    for target, (result_str, is_correct) in custom_results.items():
        if not is_correct:
            all_correct = False
            print(f"âŒ ç›®æ ‡ {target}: {result_str}")
        else:
            print(f"âœ… ç›®æ ‡ {target}: {result_str}")
    
    if all_correct:
        print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰ 10 ä¸ªæ‰‹å†™æ•°å­—å‡è¢«æ­£ç¡®è¯†åˆ«ã€‚")
    else:
        print("\nâš ï¸ è‡³å°‘æœ‰ä¸€ä¸ªæ•°å­—è¯†åˆ«é”™è¯¯ã€‚å¯èƒ½éœ€è¦è°ƒæ•´å›¾åƒé¢„å¤„ç†æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚")

# --- ä¸»ç¨‹åºå…¥å£ ---


if __name__ == '__main__':
    # 1. æ•°æ®é›†åŠ è½½
    x_train, y_train, x_test, y_test = load_and_preprocess_data()
    
    # 2. æ¨¡å‹æ„å»º
    # è¾“å…¥å½¢çŠ¶æ˜¯ (28, 28)
    model = build_bp_model(x_train.shape[1:]) 
    
    # 3. æ¨¡å‹è®­ç»ƒ
    # æ¨èä½¿ç”¨ 10-20 ä¸ª Epochs
    train_and_evaluate_model(model, x_train, y_train, x_test, y_test, 
                             epochs=10) 
    
    # 4. æµ‹è¯•è‡ªå®šä¹‰æ‰‹å†™æ•°å­—
    test_custom_numbers(model)